{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13886,
     "status": "ok",
     "timestamp": 1690895842355,
     "user": {
      "displayName": "Vera Berger",
      "userId": "07670692616498057739"
     },
     "user_tz": 240
    },
    "id": "_M8p4vkx74Az",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13886,
     "status": "ok",
     "timestamp": 1690895842355,
     "user": {
      "displayName": "Vera Berger",
      "userId": "07670692616498057739"
     },
     "user_tz": 240
    },
    "id": "_M8p4vkx74Az",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checking for GPU availability\n",
    "\n",
    "from keras import backend as K\n",
    "print(tf.config.list_physical_devices('GPU')) # check if it's installed using gpu\n",
    "print(tf.test.gpu_device_name()) # check the same\n",
    "# print(K._get_available_gpus())\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "import sys\n",
    "\n",
    "# print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "# print(f\"Keras Version: {keras.__version__}\")\n",
    "# print()\n",
    "# print(f\"Python {sys.version}\")\n",
    "# gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "# print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kernel_sizes = [3, 5, 7, 9, 11, 13, 15, 17, 19, 21]\n",
    "dropouts = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "num_filters = [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, \n",
    "               35, 36, 37, 38, 39, 40, 41, 42, 43, 44, \n",
    "               45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
    "               55, 56, 57, 58, 59, 60, 61, 62, 63, 64]\n",
    "kernel_initializers = ['he_normal', 'random_normal']\n",
    "pooling = ['median', 'max']\n",
    "learning_rates = [0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
    "padding = ['causal', 'same']#, 'valid']\n",
    "optimizers = ['Adamax', 'Adam', 'RMSprop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparams = {'kernel_size':kernel_sizes, 'dropout':dropouts, 'num_filters':num_filters, \n",
    "               'kernel_initializer':kernel_initializers,  'learning_rates':learning_rates,\n",
    "               'padding':padding, 'optimizers':optimizers\n",
    "              }#'pooling':pooling,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13886,
     "status": "ok",
     "timestamp": 1690895842355,
     "user": {
      "displayName": "Vera Berger",
      "userId": "07670692616498057739"
     },
     "user_tz": 240
    },
    "id": "_M8p4vkx74Az",
    "tags": []
   },
   "outputs": [],
   "source": [
    "window_size=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUwUWYJa74A3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # prep functions for input into model\n",
    "def fill_time(time):\n",
    "    \"\"\"\n",
    "    This snippet was taken almost directly from lightkurve fill_gaps source code\n",
    "    Takes an array of times, finds any gaps and returns a gapless time array\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    time : array\n",
    "        Array times to fill gaps within\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    ntime : array\n",
    "        Array of times with gaps filled\n",
    "    \"\"\"\n",
    "    dt = np.nanmedian(time[1::] - time[:-1:])\n",
    "    ntime = [time[0]]\n",
    "    for t in time[1::]:\n",
    "        prevtime = ntime[-1]\n",
    "        while (t - prevtime) > 1.2 * dt: # why 1.2?\n",
    "            ntime.append(prevtime + dt)\n",
    "            prevtime = ntime[-1]\n",
    "        ntime.append(t)\n",
    "    return np.asarray(ntime, float)\n",
    "# same as current func on git\n",
    "\n",
    "def fill_gaps(time, ntime, attr, zeros=False):\n",
    "    \"\"\"\n",
    "    Adapted from lightkurve fill_gaps source code\n",
    "    This is kind of janky, could just have a wrapper for fill_time \n",
    "    where if attr is none than we just fill time\n",
    "    and if attr is an actual array we fill time and the gaps in the attribute\n",
    "    the only thing is it's repetitively calling fill_time \n",
    "    whereas now it just takes an output from fill_time every time it's called\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    time : array\n",
    "        Original array of times without gaps filled in\n",
    "    ntime : array\n",
    "        Filled array of times\n",
    "    attr : array\n",
    "        Original array of some values defined at `time` times\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    nattr : array\n",
    "        Array of values with length ntime\n",
    "    \"\"\"\n",
    "    s = set(time)\n",
    "    in_original = np.asarray([I in s for I in ntime])\n",
    "    nattr = np.zeros(len(ntime)) # initialize new attribute array\n",
    "    nattr[in_original] = np.copy(attr) # copy input array\n",
    "    if zeros == False:\n",
    "        nattr[~in_original] = np.interp(ntime[~in_original].astype(np.float64), time.astype(np.float64), attr.astype(np.float64))\n",
    "    else:\n",
    "        nattr[~in_original] = 0\n",
    "    return nattr\n",
    "\n",
    "    # # version from git:\n",
    "    # in_original = np.in1d(ntime, time)\n",
    "    # nattr = np.zeros(len(ntime)) # initialize new attribute array\n",
    "    # nattr[in_original] = np.copy(attr) # copy input array\n",
    "    # if zeros == False:\n",
    "    #     nattr[~in_original] = np.interp(ntime[~in_original], time, attr)\n",
    "    # else:\n",
    "    #     nattr[~in_original] = 0\n",
    "    # return nattr\n",
    "\n",
    "\n",
    "# when e.g. tess is observing but e stray light, data time could exist but no flux\n",
    "# if more than 80% of fluxes in window are nan, don't use those\n",
    "def pad(attr, window_size=window_size, zeros=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    attr : array\n",
    "        Array to pad\n",
    "    window_size : int\n",
    "        Total number of points to pad attr by,\n",
    "        with window_size // 2 points on each end\n",
    "    zeros : bool\n",
    "        Pad with zeros if True. \n",
    "        Default is to pad with the first and last values of attr\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "         : array\n",
    "        Padded array of values\n",
    "    \"\"\"\n",
    "    if zeros == False:\n",
    "        attr_left = np.full((window_size//2,), np.nanmedian(attr[:window_size]))\n",
    "        attr_right = np.full((window_size//2,), np.nanmedian(attr[-window_size:]))\n",
    "        return np.hstack([attr_left, attr, attr_right])\n",
    "    elif zeros == True:\n",
    "        padding = np.full((window_size//2,), 0)\n",
    "        return np.hstack([padding, attr, padding])\n",
    "    else:\n",
    "        return ValueError(\"That is not a valid entry for the `zeros` parameter. Please enter True or False\")\n",
    "# Same as on git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 127,
     "status": "ok",
     "timestamp": 1690896137605,
     "user": {
      "displayName": "Vera Berger",
      "userId": "07670692616498057739"
     },
     "user_tz": 240
    },
    "id": "TkouZFX074A4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prep_for_generator(target_data, fname=None, window_size=window_size):\n",
    "    flux_list = []\n",
    "    label_list = []\n",
    "    centroid_list = []\n",
    "    poscorr_list = []\n",
    "    cr_list = []\n",
    "    lc_time = target_data[0]\n",
    "    lc_flux = target_data[-1] # injected flux array\n",
    "    \n",
    "#       lc_ferr = target_data[2] # flux error\n",
    "    lc_quality = target_data[3] # quality\n",
    "    lc_poscorr = target_data[4] # pos corr\n",
    "    lc_cdist = target_data[5] # magnitude of centroid shift\n",
    "    lc_crArr = target_data[6] # cosmic ray array\n",
    "    lc_flareArr = target_data[-2] # flare array\n",
    "    lc_flux = np.nan_to_num(lc_flux, np.nanmedian(lc_flux))\n",
    "    lc_poscorr = np.nan_to_num(lc_poscorr, 0)\n",
    "    lc_cdist = np.nan_to_num(lc_cdist, 0)\n",
    "    lc_crArr = np.nan_to_num(lc_crArr, 0)\n",
    "    lc_flareArr = np.nan_to_num(lc_flareArr, 0)\n",
    "\n",
    "    # print('got all the stuff')\n",
    "#     plt.plot(lc_time[:5000], lc_flux[:5000])\n",
    "#     plt.plot(lc_time[:1000], lc_poscorr[:1000])\n",
    "#     plt.plot(lc_time[:1000], lc_cdist[:1000])\n",
    "#     plt.plot(lc_time[:5000], lc_flareArr[:5000])\n",
    "#     plt.show()\n",
    "#         print(lc_flux)\n",
    "    # print('getting nan indices')\n",
    "    # nan_indices = np.where(np.isnan(lc_time) | np.isnan(lc_flux) | np.isnan(lc_quality) | np.isnan(lc_poscorr) | np.isnan(lc_cdist) | np.isnan(lc_crArr) | np.isnan(lc_flareArr) )[0]\n",
    "    # # # print(nan_indices)\n",
    "    # # a=100\n",
    "    # # k=200\n",
    "    # # plt.plot(lc_time[:k], np.nan_to_num(lc_flux[:k], nan=0))\n",
    "    # # plt.plot(lc_time[:k]+1, lc_flux[:k]+1)\n",
    "    # # plt.plot(lc_time[:k], np.full(k,0))\n",
    "    # print('getting rid of nans')\n",
    "    # lc_time = lc_time[~nan_indices]\n",
    "    # lc_flux = lc_flux[~nan_indices]\n",
    "    # lc_quality = lc_quality[~nan_indices]\n",
    "    # lc_poscorr = lc_poscorr[~nan_indices]\n",
    "    # lc_cdist = lc_cdist[~nan_indices]\n",
    "    # lc_crArr = lc_crArr[~nan_indices]\n",
    "    # lc_flareArr= lc_flareArr[~nan_indices]\n",
    "# \n",
    "\n",
    "    ntime = fill_time(lc_time)\n",
    "\n",
    "    nflux = fill_gaps(lc_time, ntime, lc_flux)\n",
    "    ncentroid = fill_gaps(lc_time, ntime, lc_cdist)\n",
    "    nposcorr = fill_gaps(lc_time, ntime, lc_poscorr)\n",
    "    ncr = fill_gaps(lc_time, ntime, lc_crArr)\n",
    "    labels = fill_gaps(lc_time, ntime, lc_flareArr, zeros=True)\n",
    "\n",
    "    nflux = pad(nflux, window_size=window_size, zeros=False)\n",
    "    labels = pad(labels, window_size=window_size, zeros=True)\n",
    "    ncentroid = pad(ncentroid, window_size=window_size)\n",
    "    nposcorr = pad(nposcorr, window_size=window_size)\n",
    "    ncr = pad(ncr, window_size=window_size, zeros=True)\n",
    "\n",
    "\n",
    "    if fname is not None:\n",
    "        root = str(fname).split('/')\n",
    "        root = root[-1]\n",
    "        df = pd.DataFrame(data=[ntime, nflux, nposcorr, ncentroid, ncr, labels]).T\n",
    "        np.save('filled_padded_data/'+root, np.asarray(df))\n",
    "\n",
    "    # print('LENGTHS', len(labels), len(nflux))\n",
    "    label_list.append(labels)\n",
    "    flux_list.append(nflux)\n",
    "    centroid_list.append(ncentroid)\n",
    "    poscorr_list.append(nposcorr)\n",
    "    cr_list.append(ncr)\n",
    "#         print(len(label_list))\n",
    "#         print(len(flux_list))\n",
    "#     print('plotting')\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     k = 600\n",
    "#     plotflux = nflux[:k]\n",
    "#     plottime = ntime[:k]\n",
    "#     plt.plot(lc_time[:k], lc_flux[:k], label='orig')\n",
    "#     plt.plot(plottime, plotflux, color='black', label='filled padded labelled')\n",
    "# #         print('final num of flare pts', len(labels[labels==1]))\n",
    "# #         print('final num of nonflare pts', len(labels[labels==0]))\n",
    "#     plt.plot(plottime[labels[:k]==1], plotflux[labels[:k]==1], color='red', marker='o', ls='none', markersize=2)\n",
    "#     plt.show()\n",
    "    print(' fluxes', len(flux_list), 'labels', len(label_list), 'centroid', len(centroid_list), 'poscorr', len(poscorr_list))\n",
    "    return flux_list, label_list, centroid_list, poscorr_list, cr_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fname = '/Users/vberger/NASA/flarenet/training_data/259130275_56_2_data.npy'\n",
    "fname = 'training_data/17232188_43_1_data.npy'\n",
    "\n",
    "\n",
    "# fname = '/Users/veraberger/nasa/training_data/257004995_47_1_data.npy'\n",
    "x = np.load(fname, allow_pickle=True).T\n",
    "t=x[0]\n",
    "ntime = fill_time(t)\n",
    "# ntime = pad(ntime1)\n",
    "\n",
    "# plt.plot(x[0][:1000], x[-1][:1000])\n",
    "\n",
    "\n",
    "# print(x[0].min(), ntime.min())\n",
    "\n",
    "# np.any(np.isnan(x[1]))\n",
    "flux_list, label_list, centroid_list, poscorr_list, cr_list = prep_for_generator(x)\n",
    "\n",
    "# np.any(np.isnan(flux_list))\n",
    "[a] = flux_list\n",
    "\n",
    "print(len(ntime), len(a), len(x[-1]))\n",
    "# print('filled padded')\n",
    "# for elt in a[-100:]:\n",
    "#     print(elt)\n",
    "# print('ntime')\n",
    "# for elt in ntime[-100:]:\n",
    "#     print(elt)\n",
    "# # print(flux_list[-100:])\n",
    "# print('orig ')\n",
    "# print(x[-1][-100:])\n",
    "\n",
    "# plt.plot(x[0], x[-1]+.25, label='orig')\n",
    "# plt.plot(ntime, a, label='filled padded')\n",
    " \n",
    "k=500\n",
    "# plt.plot(x[0][:k], x[-1][:k]+.5, label='orig')\n",
    "# plt.plot(ntime[:k], a[:k], label='filled padded')\n",
    "\n",
    "# plt.plot( x[-1][:k]+.5, label='orig')\n",
    "# plt.plot( a[:k], label='filled padded')\n",
    "\n",
    "# plt.plot( x[-1][-k:]+.5, label='orig')\n",
    "# plt.plot( a[-k:], label='filled padded')\n",
    "\n",
    "# plt.plot( x[-1][:k]+.5, label='orig')\n",
    "# plt.plot( a[:k], label='filled padded')\n",
    "\n",
    "plt.plot( x[-1]+.5, label='orig')\n",
    "plt.plot( a, label='filled padded')\n",
    "\n",
    "\n",
    "\n",
    "# print(ntime[:k])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# if the filled padded one looks shorter its because we are plotting the same number of pts for each so more pts are \n",
    "# going into what was simply undefined in the original lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7yWO1Jm574A5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def multiple_training_data_generator(file_list, batch_size=32, window_size=window_size, drop_frac=0.5,  train=True):\n",
    "    for file in file_list:\n",
    "        print(file)\n",
    "        target_data = np.load(file, allow_pickle=True).T\n",
    "        lc_list, label_list, centroid_list, poscorr_list, cr_list = prep_for_generator(target_data, file)\n",
    "        i = 0\n",
    "        print('length of lc list', len(lc_list))\n",
    "        while i < len(lc_list):\n",
    "            target_lc = lc_list[i]\n",
    "            centroid_lc = centroid_list[i]\n",
    "            poscorr_lc = poscorr_list[i]\n",
    "            cr_lc = cr_list[i]\n",
    "            labels = label_list[i] # flare array\n",
    "\n",
    "            lc_length = len(target_lc)\n",
    "            valid_indices = np.arange(int(window_size/2), int(lc_length-window_size/2), dtype=int) \n",
    "            \n",
    "            # choose indices randomly to include in the batches\n",
    "            if train:\n",
    "                # print('getting valid indices')\n",
    "                # valid_indices = np.random.choice(valid_indices, size=round(len(valid_indices)*drop_frac), replace=False)\n",
    "                valid_lc = target_lc[valid_indices]\n",
    "                valid_labels = labels[valid_indices]\n",
    "                flare_indices = valid_indices[valid_labels==1]\n",
    "                nonflare_indices = valid_indices[valid_labels==0]\n",
    "                size =  min(len(nonflare_indices), len(flare_indices))\n",
    "                # include many/all CR points in nonflare set to not lose cr info\n",
    "                # even flare and notflare&cr\n",
    "\n",
    "                # valid indices are drawn from half flares and half non flares\n",
    "                valid_flare_indices = np.random.choice(flare_indices, size=math.floor(size*drop_frac), replace=False)\n",
    "                valid_nonflare_indices = np.random.choice(nonflare_indices, size=math.floor(size*drop_frac), replace=False)\n",
    "                valid_indices = np.concatenate((valid_flare_indices, valid_nonflare_indices))\n",
    "                np.random.shuffle(valid_indices)\n",
    "                # print(valid_indices)\n",
    "                # plt.figure(figsize=(10,8))\n",
    "                # plt.plot(range(len(target_lc)), target_lc)\n",
    "                # plt.plot(valid_flare_indices, target_lc[valid_flare_indices], ls='none', marker='o', c='g')\n",
    "                # plt.plot(valid_nonflare_indices, target_lc[valid_nonflare_indices], ls='none', marker='o', c='r')\n",
    "                # plt.show()\n",
    "                \n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            \n",
    "            j = 0 #J loops through the time series for a single target\n",
    "            \n",
    "            \n",
    "            while j+batch_size <= len(valid_indices):\n",
    "                data = np.empty((batch_size, window_size, 1)) # lc\n",
    "                data_centroid = np.empty((batch_size, window_size, 1)) # centroid\n",
    "                data_poscorr = np.empty((batch_size, window_size, 1)) # poscorr\n",
    "                data_cr = np.empty((batch_size, window_size, 1)) # crs\n",
    "                label = np.empty((batch_size), dtype=int)\n",
    "                # generate a batch\n",
    "                for k in range(batch_size):\n",
    "\n",
    "#                     make an lc centered at valid_indices[j+k] of length window_size\n",
    "                    X = target_lc[valid_indices[j+k]-int(window_size/2) : valid_indices[j+k]+int(window_size/2)].reshape(window_size,1)\n",
    "                    data[k,] = RobustScaler().fit_transform(X)\n",
    "                    label[k] = np.asarray(labels[valid_indices[j+k]]) # marks whether or not the point at j+k is part of a flare\n",
    "                    X2 = centroid_lc[valid_indices[j+k]-int(window_size/2) : valid_indices[j+k]+int(window_size/2)].reshape(window_size,1)\n",
    "                    data_centroid[k,] = RobustScaler().fit_transform(X2)\n",
    "                    X3 = poscorr_lc[valid_indices[j+k]-int(window_size/2) : valid_indices[j+k]+int(window_size/2)].reshape(window_size,1)\n",
    "                    data_poscorr[k,] = RobustScaler().fit_transform(X3)\n",
    "                    X4 = cr_lc[valid_indices[j+k]-int(window_size/2) : valid_indices[j+k]+int(window_size/2)].reshape(window_size,1)\n",
    "                    data_cr[k,] = RobustScaler().fit_transform(X4)\n",
    "                yield {\"inputA\":data, \"inputB\":data_centroid, \"inputC\":data_poscorr, \"inputD\":data_cr}, label\n",
    "                # yield {\"inputA\":data}, label\n",
    "                j = j + batch_size\n",
    "            \"\"\"\n",
    "            while j+batch_size < len(valid_indices):\n",
    "                data = np.empty((batch_size, window_size, 1)) # lc\n",
    "                data_centroid = np.empty((batch_size, window_size, 1)) # centroid\n",
    "                data_poscorr = np.empty((batch_size, window_size, 1)) # poscorr\n",
    "                data_cr = np.empty((batch_size, window_size, 1)) # crs\n",
    "                label = np.empty((batch_size), dtype=int)\n",
    "                k = 0\n",
    "                \n",
    "                # generate a batch\n",
    "                # for k in range(batch_size):\n",
    "                while ((k < batch_size) & (j+batch_size < len(valid_indices))):\n",
    "#                     make an lc centered at valid_indices[j+k] of length window_size\n",
    "                    # print('j+k', j+k, ', len of valid indices', len(valid_indices), ', j + batch size', j+batch_size)\n",
    "                    X = target_lc[valid_indices[j+k]-int(window_size/2) : valid_indices[j+k]+int(window_size/2)].reshape(window_size,1)\n",
    "                    \n",
    "                    if np.mean(np.isnan(X)) < 0.2:\n",
    "                        # fill NaNs\n",
    "                        np.nan_to_num(X, nan=np.median(X), copy=False)\n",
    "                        data[k,] = RobustScaler().fit_transform(X)\n",
    "                        # for x in data[k,]:\n",
    "                            # print(x)\n",
    "                        # print(np.sum(np.isnan(X)))\n",
    "                        label[k] = np.asarray(labels[valid_indices[j+k]]) # marks whether or not the point at j+k is part of a flare\n",
    "                        X2 = centroid_lc[valid_indices[j+k]-int(window_size/2) : valid_indices[j+k]+int(window_size/2)].reshape(window_size,1)\n",
    "                        data_centroid[k,] = RobustScaler().fit_transform(X2)\n",
    "                        X3 = poscorr_lc[valid_indices[j+k]-int(window_size/2) : valid_indices[j+k]+int(window_size/2)].reshape(window_size,1)\n",
    "                        data_poscorr[k,] = RobustScaler().fit_transform(X3)\n",
    "                        X4 = cr_lc[valid_indices[j+k]-int(window_size/2) : valid_indices[j+k]+int(window_size/2)].reshape(window_size,1)\n",
    "                        data_cr[k,] = RobustScaler().fit_transform(X4)\n",
    "                        #     # # # UNCOMMENT THIS TO CHECK LABELLING ## \n",
    "\n",
    "                        # plt.plot(data_poscorr[k]-3, color='c')\n",
    "                        # plt.plot(data_centroid[k]+5, color='g')\n",
    "                        # plt.plot(data_cr[k], color='y')\n",
    "                        # plt.axvline(window_size/2, color='b')\n",
    "                        # plt.plot(data[k], color='black', markerfacecolor='orange', marker='o', markersize=2, linewidth=0.4)\n",
    "\n",
    "                        # plt.title(label[k])\n",
    "                        # plt.plot(labels[valid_indices[j+k]-int(window_size/2) : valid_indices[j+k]+int(window_size/2)], c='r')\n",
    "                        # plt.show()\n",
    "                        \n",
    "                        k += 1\n",
    "                    j +=1\n",
    "                # check they all have values, otherwise don't yield\n",
    "                # if not np.any(np.isnan(data), axis=1):\n",
    "                is_empty_inner = np.any(np.isnan(data), axis=1)\n",
    "                empty_indices = np.where(is_empty_inner)[0]\n",
    "                if len(empty_indices) == 0:\n",
    "                    # yield {\"inputA\":data, \"inputB\":data_centroid, \"inputC\":data_poscorr, \"inputD\":data_cr}, label\n",
    "                    yield {\"inputA\":data}, label\n",
    "                \"\"\"\n",
    "\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Se9wvDlr74A6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "file_list = glob.glob('/Users/vberger/NASA/flarenet/training_data/*.npy')\n",
    "len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAgr1Mkh74A6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = multiple_training_data_generator(file_list, batch_size=32, window_size=window_size)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0l1LGglO74A7",
    "outputId": "75d76bf9-361c-4b23-e6ea-43f6815a8b35",
    "tags": []
   },
   "outputs": [],
   "source": [
    "num = 0\n",
    "\n",
    "\n",
    "for data, label in test_data:\n",
    "     print(data['inputA'].shape)\n",
    "    # print(data['inputA'].shape, data['inputB'].shape, data['inputC'].shape, data['inputD'].shape, label.shape)\n",
    "     print(label, \"<--Labels\")\n",
    "     print()\n",
    "     num = num + 1\n",
    "     if num >5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDRTRDQP74A7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a basic ML that we can use to make sure we know the data is being imported correctly\n",
    "def basic_model():\n",
    "    \n",
    "    # There is 1 input light curve. In this case, we're just looking at the flux 1D time series\n",
    "    inputA = keras.layers.Input(shape=(window_size,1), name='inputA') # flux lc\n",
    "    inputB = keras.layers.Input(shape=(window_size,1), name='inputB') # centroid\n",
    "    inputC = keras.layers.Input(shape=(window_size,1), name='inputC') # pos corr\n",
    "    inputD = keras.layers.Input(shape=(window_size,1), name='inputD') # cosmic rays\n",
    "\n",
    "    # Convolutions on the flux lightcurve\n",
    "    A = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(inputA)\n",
    "    A = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(A)\n",
    "    A = keras.layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(A)\n",
    "    A = keras.layers.Dropout(0.2)(A)\n",
    "    # A = keras.layers.BatchNormalization()(A) # check on if these just need to be at the end\n",
    "    # 2\n",
    "    A = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(A)\n",
    "    A = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(A)\n",
    "    A = keras.layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(A)\n",
    "    A = keras.layers.Dropout(0.2)(A)\n",
    "    A = keras.layers.BatchNormalization()(A)\n",
    "    # # 3\n",
    "    # A = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(A)\n",
    "    # A = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(A)\n",
    "    # A = keras.layers.MaxPooling1D(pool_size=3, strides=2,padding='same')(A)\n",
    "    # A = keras.layers.Dropout(0.2)(A)\n",
    "    # # A = keras.layers.BatchNormalization()(A)\n",
    "    # # 4\n",
    "    # A = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(A)\n",
    "    # A = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(A)\n",
    "    # A = keras.layers.MaxPooling1D(pool_size=3, strides=2,padding='same')(A)\n",
    "    # A = keras.layers.Dropout(0.2)(A)\n",
    "    # # A = keras.layers.BatchNormalization()(A)\n",
    "    # # 5\n",
    "    # A = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(A)\n",
    "    # A = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(A)\n",
    "    # A = keras.layers.MaxPooling1D(pool_size=3, strides=2,padding='same')(A)\n",
    "    # A = keras.layers.Dropout(0.2)(A)   \n",
    "    # # A = keras.layers.BatchNormalization()(A)\n",
    "    # # 6\n",
    "    # A = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(A)\n",
    "    # A = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(A)\n",
    "    # A = keras.layers.MaxPooling1D(pool_size=3, strides=2,padding='same')(A)\n",
    "    # A = keras.layers.Dropout(0.2)(A)\n",
    "    # # A = keras.layers.BatchNormalization()(A)\n",
    "    # # 7\n",
    "    # A = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(A)\n",
    "    # A = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(A)\n",
    "    # A = keras.layers.MaxPooling1D(pool_size=3, strides=2,padding='same')(A)\n",
    "    # A = keras.layers.Dropout(0.2)(A)\n",
    "    # # A = keras.layers.BatchNormalization()(A)\n",
    "    # # 8\n",
    "    # A = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(A)\n",
    "    # A = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(A)\n",
    "    # A = keras.layers.MaxPooling1D(pool_size=2, strides=2,padding='same')(A)\n",
    "    # A = keras.layers.Dropout(0.2)(A)\n",
    "    # # A = keras.layers.BatchNormalization()(A)\n",
    "    # # 9\n",
    "    # A = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(A)\n",
    "    # A = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(A)\n",
    "    # A = keras.layers.MaxPooling1D(pool_size=2, strides=2,padding='same')(A)\n",
    "    # A = keras.layers.Dropout(0.2)(A)  \n",
    "    # # A = keras.layers.BatchNormalization()(A)\n",
    "    # # 10\n",
    "    # A = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(A)\n",
    "    # A = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(A)\n",
    "    # A = keras.layers.MaxPooling1D(pool_size=2, strides=2,padding='same')(A)\n",
    "    # A = keras.layers.Dropout(0.2)(A)  \n",
    "    # # A = keras.layers.BatchNormalization()(A)\n",
    "\n",
    "    A = keras.layers.Flatten()(A)\n",
    "    A = keras.models.Model(inputA, A)\n",
    "\n",
    "    # centroid \n",
    "    B = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(inputB)\n",
    "    B = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(B)\n",
    "    B = keras.layers.MaxPooling1D(pool_size=3, strides=2)(B)\n",
    "    B = keras.layers.Dropout(0.2)(B)\n",
    "    \n",
    "    B = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(B)\n",
    "    B = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(B)\n",
    "    B = keras.layers.MaxPooling1D(pool_size=3, strides=2)(B)\n",
    "    B = keras.layers.Dropout(0.2)(B)\n",
    "    B = keras.layers.Flatten()(B)\n",
    "    B = keras.models.Model(inputB, B)\n",
    "\n",
    "    # # pos corr\n",
    "    C = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(inputC)\n",
    "    C = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(C)\n",
    "    C = keras.layers.MaxPooling1D(pool_size=3, strides=2)(C)\n",
    "    C = keras.layers.Dropout(0.2)(C)\n",
    "    \n",
    "    C = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(C)\n",
    "    C = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(C)\n",
    "    C = keras.layers.MaxPooling1D(pool_size=3, strides=2)(C)\n",
    "    C = keras.layers.Dropout(0.2)(C)\n",
    "    C = keras.layers.Flatten()(C)\n",
    "    C = keras.models.Model(inputC, C)\n",
    "\n",
    "    # # cosmic rays\n",
    "    D = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(inputD)\n",
    "    D = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(D)\n",
    "    D = keras.layers.MaxPooling1D(pool_size=3, strides=2)(D)\n",
    "    D = keras.layers.Dropout(0.2)(D)\n",
    "    \n",
    "    D = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(inputD)\n",
    "    D = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"causal\", activation='relu')(D)\n",
    "    D = keras.layers.MaxPooling1D(pool_size=3, strides=2)(D)\n",
    "    D = keras.layers.Dropout(0.2)(D)\n",
    "    D = keras.layers.Flatten()(D)\n",
    "    D = keras.models.Model(inputD, D)\n",
    "\n",
    "    # Combine the two convolution branches before entering the dense neural network layers\n",
    "    combined = keras.layers.concatenate([A.output, B.output, C.output, D.output])\n",
    "    # combined=A.output\n",
    "\n",
    "    # Final fully connected layers to make the prediction\n",
    "    # Note that the final layer has an output shape of 1. This is because it will be a single prediction between 0 and 1\n",
    "    F = keras.layers.Dense(512, activation='relu', kernel_initializer='he_normal')(combined) # look at size of input to this layer, if smaller than 512 maybe make 512 smaller. look at what stella does too\n",
    "    F = keras.layers.Dropout(0.2)(F)\n",
    "    F = keras.layers.Dense(512, activation='relu', kernel_initializer='he_normal')(F)\n",
    "    F = keras.layers.Dropout(0.2)(F)\n",
    "    F = keras.layers.Dense(1, activation='sigmoid')(F)\n",
    "    # keep the relu -> sigmoid\n",
    "\n",
    "\n",
    "    multi_layer_model = keras.models.Model(inputs=(inputA, inputB, inputC, inputD), outputs=(F))\n",
    "    # multi_layer_model = keras.models.Model(inputs=(inputA), outputs=(F))\n",
    "\n",
    "    multi_layer_model.compile(\n",
    "    optimizer=keras.optimizers.legacy.Adamax(learning_rate=0.001),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return multi_layer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gridsearch_model(dropout, kernel_initializer, kernel_size, learning_rate, num_filters, optimizer, padding):\n",
    "    # can add params to dict that are specific to each input \n",
    "    \n",
    "    # There is 1 input light curve. In this case, we're just looking at the flux 1D time series\n",
    "    inputA = keras.layers.Input(shape=(window_size,1), name='inputA') # flux lc\n",
    "    inputB = keras.layers.Input(shape=(window_size,1), name='inputB') # centroid\n",
    "    inputC = keras.layers.Input(shape=(window_size,1), name='inputC') # pos corr\n",
    "    inputD = keras.layers.Input(shape=(window_size,1), name='inputD') # cosmic rays\n",
    "\n",
    "    # Convolutions on the flux lightcurve\n",
    "    A = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(inputA)\n",
    "    A = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(A)\n",
    "    A = keras.layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(A)\n",
    "    A = keras.layers.Dropout(dropout)(A)\n",
    "    # A = keras.layers.BatchNormalization()(A) # check on if these just need to be at the end\n",
    "    # 2\n",
    "    A = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(A)\n",
    "    A = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(A)\n",
    "    A = keras.layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(A)\n",
    "    A = keras.layers.Dropout(dropout)(A)\n",
    "    # A = keras.layers.BatchNormalization()(A)\n",
    "    # 3\n",
    "    A = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(A)\n",
    "    A = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(A)\n",
    "    A = keras.layers.MaxPooling1D(pool_size=3, strides=2,padding='same')(A)\n",
    "    A = keras.layers.Dropout(dropout)(A)\n",
    "    # A = keras.layers.BatchNormalization()(A)\n",
    "    # 4\n",
    "    A = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(A)\n",
    "    A = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(A)\n",
    "    A = keras.layers.MaxPooling1D(pool_size=3, strides=2,padding='same')(A)\n",
    "    A = keras.layers.Dropout(dropout)(A)\n",
    "    # A = keras.layers.BatchNormalization()(A)\n",
    "    # 5\n",
    "    A = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(A)\n",
    "    A = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(A)\n",
    "    A = keras.layers.MaxPooling1D(pool_size=3, strides=2,padding='same')(A)\n",
    "    A = keras.layers.Dropout(dropout)(A)   \n",
    "    # A = keras.layers.BatchNormalization()(A)\n",
    "    # 6\n",
    "    A = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(A)\n",
    "    A = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(A)\n",
    "    A = keras.layers.MaxPooling1D(pool_size=3, strides=2,padding='same')(A)\n",
    "    A = keras.layers.Dropout(dropout)(A)\n",
    "    # A = keras.layers.BatchNormalization()(A)\n",
    "    # 7\n",
    "    A = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(A)\n",
    "    A = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(A)\n",
    "    A = keras.layers.MaxPooling1D(pool_size=3, strides=2,padding='same')(A)\n",
    "    A = keras.layers.Dropout(dropout)(A)\n",
    "    # A = keras.layers.BatchNormalization()(A)\n",
    "    # 8\n",
    "    A = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(A)\n",
    "    A = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(A)\n",
    "    A = keras.layers.MaxPooling1D(pool_size=2, strides=2,padding='same')(A)\n",
    "    A = keras.layers.Dropout(dropout)(A)\n",
    "    # A = keras.layers.BatchNormalization()(A)\n",
    "    # 9\n",
    "    A = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(A)\n",
    "    A = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(A)\n",
    "    A = keras.layers.MaxPooling1D(pool_size=2, strides=2,padding='same')(A)\n",
    "    A = keras.layers.Dropout(dropout)(A)  \n",
    "    # A = keras.layers.BatchNormalization()(A)\n",
    "    # 10\n",
    "    A = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(A)\n",
    "    A = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(A)\n",
    "    A = keras.layers.MaxPooling1D(pool_size=2, strides=2,padding='same')(A)\n",
    "    A = keras.layers.Dropout(dropout)(A)  \n",
    "    # A = keras.layers.BatchNormalization()(A)\n",
    "\n",
    "    A = keras.layers.Flatten()(A)\n",
    "    A = keras.models.Model(inputA, A)\n",
    "\n",
    "    # centroid \n",
    "    B = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(inputB)\n",
    "    B = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(B)\n",
    "    B = keras.layers.MaxPooling1D(pool_size=3, strides=2)(B)\n",
    "    B = keras.layers.Dropout(dropout)(B)\n",
    "    \n",
    "    B = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(B)\n",
    "    B = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(B)\n",
    "    B = keras.layers.MaxPooling1D(pool_size=3, strides=2)(B)\n",
    "    B = keras.layers.Dropout(dropout)(B)\n",
    "    B = keras.layers.Flatten()(B)\n",
    "    B = keras.models.Model(inputB, B)\n",
    "\n",
    "    # pos corr\n",
    "    C = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(inputC)\n",
    "    C = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(C)\n",
    "    C = keras.layers.MaxPooling1D(pool_size=3, strides=2)(C)\n",
    "    C = keras.layers.Dropout(dropout)(C)\n",
    "    \n",
    "    C = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(C)\n",
    "    C = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(C)\n",
    "    C = keras.layers.MaxPooling1D(pool_size=3, strides=2)(C)\n",
    "    C = keras.layers.Dropout(dropout)(C)\n",
    "    C = keras.layers.Flatten()(C)\n",
    "    C = keras.models.Model(inputC, C)\n",
    "\n",
    "    # cosmic rays\n",
    "    D = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(inputD)\n",
    "    D = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(D)\n",
    "    D = keras.layers.MaxPooling1D(pool_size=3, strides=2)(D)\n",
    "    D = keras.layers.Dropout(dropout)(D)\n",
    "    \n",
    "    D = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(inputD)\n",
    "    D = keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, padding=padding, activation='relu')(D)\n",
    "    D = keras.layers.MaxPooling1D(pool_size=3, strides=2)(D)\n",
    "    D = keras.layers.Dropout(dropout)(D)\n",
    "    D = keras.layers.Flatten()(D)\n",
    "    D = keras.models.Model(inputD, D)\n",
    "\n",
    "    # Combine the two convolution branches before entering the dense neural network layers\n",
    "    combined = keras.layers.concatenate([A.output, B.output, C.output, D.output])\n",
    "#     combined=A.output\n",
    "\n",
    "    # Final fully connected layers to make the prediction\n",
    "    # Note that the final layer has an output shape of 1. This is because it will be a single prediction between 0 and 1\n",
    "    F = keras.layers.Dense(512, activation='relu', kernel_initializer=kernel_initializer)(combined) # look at size of input to this layer, if smaller than 512 maybe make 512 smaller. look at what stella does too\n",
    "    F = keras.layers.Dropout(dropout)(F)\n",
    "    F = keras.layers.Dense(512, activation='relu', kernel_initializer=kernel_initializer)(F)\n",
    "    F = keras.layers.Dropout(dropout)(F)\n",
    "    F = keras.layers.Dense(1, activation='sigmoid')(F)\n",
    "    # keep the relu -> sigmoid\n",
    "\n",
    "\n",
    "    multi_layer_model = keras.models.Model(inputs=(inputA, inputB, inputC, inputD), outputs=(F))\n",
    "\n",
    "    multi_layer_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return multi_layer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLuCD8hI74A7",
    "outputId": "7ed131bd-7bac-4e78-c97a-8949ae3b87ea",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = basic_model()\n",
    "# model = gridsearch_model()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from keras.models import load_model \n",
    "# reconst_model = load_model('modelmon/model.h5')\n",
    "# gs=GridSearchCV(estimator=model, param_grid=params, cv=10)\n",
    "random_search = RandomizedSearchCV(estimator=gridsearch_model, param_distributions=hyperparams, n_iter=3, cv=10) # make sure it runs on my laptop, then make it ~100 for supercomputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RFprl-I974A7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(multiple_training_data_generator, args=[file_list[:300], window_size],\n",
    "                                         output_types = ({\"inputA\": tf.int32, \"inputB\": tf.int32, \"inputC\": tf.int32, \"inputD\": tf.int32}, tf.float32),\n",
    "                                            output_shapes = ({\"inputA\":(None,window_size,1),\"inputB\":(None,window_size,1),\"inputC\":(None,window_size,1), \"inputD\":(None,window_size,1)},\n",
    "                                                             (None,)))\n",
    "# dataset = tf.data.Dataset.from_generator(multiple_training_data_generator, args=[file_list[:200], window_size],\n",
    "#                                          output_types = ({\"inputA\": tf.int32}, tf.float32),\n",
    "#                                             output_shapes = ({\"inputA\":(None,window_size,1)},(None,)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTEisex-74A7",
    "outputId": "59802c6b-570d-4c40-f07a-7acbc677c20a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This callback will stop the training when there is no improvement in the loss for three consecutive epochs.\n",
    "callback = keras.callbacks.EarlyStopping(monitor='loss', patience=3) # can up it, esp for final model. not sm for cross validation\n",
    "# make sure it saves num epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparams = {'kernel_size':kernel_sizes, 'dropout':dropouts, 'num_filters':num_filters, \n",
    "#                'kernel_initializer':kernel_initializers,  'learning_rates':learning_rates,\n",
    "#                'padding':padding, 'optimizers':optimizers\n",
    "#               }#'pooling':pooling,\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "params = list(ParameterGrid(hyperparams))\n",
    "np.random.shuffle(params) # choose a subset\n",
    "j=0\n",
    "for d in params[:10]:\n",
    "    # print(d)\n",
    "    j+=1\n",
    "    model = gridsearch_model(d['dropout'], d['kernel_initializer'], d['kernel_size'],\n",
    "                            d['learning_rates'], d['num_filters'], d['optimizers'], d['padding'])\n",
    "\n",
    "    history = model.fit(dataset, epochs=10, callbacks=callback)\n",
    "    \n",
    "    # just save the dict of hyperparams d and the statistics on it bc then can recreate best model\n",
    "    # save history -- \n",
    "    # dict with history and all the params, and save that\n",
    "\n",
    "    \n",
    "    # plot loss\n",
    "    # train_loss = history.history['loss']\n",
    "    # train_acc = history.history['accuracy']\n",
    "    # plt.plot(train_loss)\n",
    "    # plt.xlabel('Epoch', size=18)\n",
    "    # plt.xlabel('Loss', size=18)\n",
    "    # plt.savefig('figures/'+str(j)+'_loss.png', dpi=200)\n",
    "    # plt.clf()\n",
    "    # # plot accuracy\n",
    "    # plt.plot(train_acc, color='tab:green')\n",
    "    # plt.xlabel('Epoch', size=18)\n",
    "    # plt.xlabel('Accuracy', size=18)\n",
    "    # plt.savefig('figures/'+str(j)+'_accuracy.png', dpi=200)\n",
    "    # plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjy0gtyg74A7",
    "outputId": "f288ebe0-30a5-4730-df74-22a42ce93b3f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(dataset, epochs=20)#, callbacks=callback)#, use_multiprocessing=True)\n",
    "# history = random_search.fit(dataset, epochs=10)#, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "# model_json = model.to_json()\n",
    "# with open(\"model1220.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "# # serialize weights to HDF5\n",
    "\n",
    "# model.save_weights(\"model.h5\")\n",
    "model.save('model_0613.h5')\n",
    "# print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_06_13_24.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reconstructed_model = keras.models.load_model(\"model2/model30.keras\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = history.history['loss']\n",
    "train_acc = history.history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_loss)\n",
    "plt.plot(train_loss)\n",
    "plt.xlabel('Epoch', size=18)\n",
    "\n",
    "plt.xlabel('Loss', size=18)\n",
    "\n",
    "# plt.savefig('06_13_2024_loss_allinputs.png', dpi=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_acc, color='tab:green')\n",
    "plt.xlabel('Epoch', size=18)\n",
    "plt.xlabel('Accuracy', size=18)\n",
    "# plt.savefig('06_13_2024_accuracy_allinputs.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2M4bqzTD74A5"
   },
   "outputs": [],
   "source": [
    "file_list_pred = file_list[300]\n",
    "lcdata = np.load(file_list_pred, allow_pickle=True).T\n",
    "flux_list, label_list, centroid_list, poscorr_list, cr_list = prep_for_generator(lcdata, fname=None)\n",
    "\n",
    "[sub_lc] =flux_list\n",
    "[sub_labels] =label_list\n",
    "[sub_centroid] = centroid_list\n",
    "[sub_poscorr] = poscorr_list\n",
    "[sub_cr] = cr_list\n",
    "init_time = lcdata[0]\n",
    "\n",
    "k = 3000\n",
    "# take a k-sized section of the lc\n",
    "sub_lc = sub_lc[:k]\n",
    "sub_labels = sub_labels[:k]\n",
    "sub_centroid = sub_centroid[:k]\n",
    "sub_poscorr = sub_poscorr[:k]\n",
    "sub_cr = sub_cr[:k]\n",
    "plt.plot(sub_lc)\n",
    "plt.plot(sub_labels)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # I haven't actually tested this\n",
    "# score = model.evaluate([sub_lc, sub_centroid, sub_poscorr, sub_cr], verbose=0)\n",
    "# print('Test loss:', score[0])\n",
    "# print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntime = fill_time(init_time)\n",
    "nflares = fill_gaps(init_time, ntime, flares, zeros=True)\n",
    "nflares = pad(nflares)\n",
    "nflares = nflares[:5500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sub_lc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install ipywidgets --yes\n",
    "# !conda install jupyter --yes\n",
    "\n",
    "\n",
    "preds = np.zeros(len(sub_lc))\n",
    "print(len(preds))\n",
    "for i in range(len(sub_lc)-window_size):\n",
    "# for i in tqdm(range(len(sub_lc)-window_size)):\n",
    "    lc_slice = sub_lc[i:i+window_size]\n",
    "    centroid_slice = sub_centroid[i:i+window_size]\n",
    "    poscorr_slice = sub_poscorr[i:i+window_size]\n",
    "    cr_slice = sub_cr[i:i+window_size]\n",
    "    preds[i+int(window_size/2)] = model.predict([lc_slice.reshape(1,window_size,1), \n",
    "                                                 centroid_slice.reshape(1,window_size,1), \n",
    "                                                poscorr_slice.reshape(1,window_size,1), \n",
    "                                                cr_slice.reshape(1,window_size,1)], \n",
    "                                                verbose=0\n",
    "                                               )\n",
    "    # # if using only the flux time series as input\n",
    "    # preds[i+int(window_size/2)] = model.predict([lc_slice.reshape(1,window_size,1)], \n",
    "    #                                             verbose=0\n",
    "    #                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "6d691ee432b04d259e8bf0f66c9bcb1d"
     ]
    },
    "id": "Fa6lGvih74A7",
    "outputId": "2bf96846-a667-4254-d62e-f495b670afd1"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# sub_lc = lc_list[0][:15000]\n",
    "# preds = np.zeros(len(sub_lc))\n",
    "# for i in tqdm(range(len(sub_lc)-window_size)):\n",
    "#   lc_slice = sub_lc[i:i+window_size]\n",
    "#   #plt.scatter(lc_slice.time.value, lc_slice.flux.value, c = y[(idx+i-int(num_preds/2)-int(window_size/2)) : (idx+i-int(num_preds/2)+int(window_size/2))])\n",
    "#   #plt.show()\n",
    "#   preds[i+int(window_size/2)] = model.predict(lc_slice.reshape(1,window_size,1), verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aWkCZsk174A7",
    "outputId": "dd1644b7-8e1d-4913-e3b4-10b83653e479"
   },
   "outputs": [],
   "source": [
    "print(sum(preds), '* max pred', preds.max(), '* min pred', preds.min(), '* median pred', np.median(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfwxn1Hl74A8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "fig, ax = plt.subplots(2, figsize=(18,11), sharex=True, sharey=True)\n",
    "ax[0].scatter(np.linspace(0, len(sub_lc), len(sub_lc)), sub_lc, c='black', s=10)\n",
    "# ax[0].scatter(np.linspace(0, len(sub_lc), len(sub_lc)), sub_lc, c=nflares, cmap='viridis', s=10)\n",
    "mappable = ax[1].scatter(np.linspace(0, (len(sub_lc)), len(sub_lc)), sub_lc, c=preds, cmap='viridis', s=100)\n",
    "plt.colorbar(mappable)\n",
    "# ax[0].set_ylim(-.03, 0.25)\n",
    "# ax[0].set_xlim(2400, 2500)\n",
    "ax[1].set_ylabel(\"Relative flux\", size=18)\n",
    "ax[1].set_xlabel(\"Time\", size=18)\n",
    "# fig.tick_params(labelsize=18)\n",
    "# ax[0].set_xlim(20000, 45000)\n",
    "# plt.savefig(file_list[10]+'.png', dpi=200, bbox_inches='tight', transparent=False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
